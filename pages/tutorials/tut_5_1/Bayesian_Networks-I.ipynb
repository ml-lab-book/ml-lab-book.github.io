{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* <a href=\"https://www.coursera.org/learn/probabilistic-graphical-models\"> Daphne Koller: Probabilistic Graphical Models: Representation </a>\n",
    "\n",
    "\n",
    "### Generative Models v/s Discriminative models:\n",
    "\n",
    "* Generative: Model the joint distribution of all the random variables (X) i.e P(X).\n",
    "\n",
    "\n",
    "* Discriminative: Model the conditional probability of some of the variables (say Y), given other ones (say X) i.e P(Y|X)\n",
    "    \n",
    "\n",
    "* **Q**. Suppose $ | Y | = n $ and $ | X | = m $. Assume all the random variables are binary. How many parameters would you have to learn if you want to build a:\n",
    "    * Discriminative model?\n",
    "    * Generative model?\n",
    "\n",
    "\n",
    "* You can obtain a discriminative model from a generative one. \n",
    "\n",
    "\n",
    "* Discriminative models involve lesser complexity than the Generative ones.\n",
    "\n",
    "\n",
    "### Why Probabilistic Models?\n",
    " \n",
    " \n",
    "   * Probabilistic models capture the uncertainty in the result as well.\n",
    " \n",
    " \n",
    "   * Some people believe that there's an inherent stochasticity in nature. So, everything is probabilistic. \n",
    " \n",
    " \n",
    "   * Probability theory has been well studied. Can utilize the Statistical learning toolbox.\n",
    "    \n",
    "\n",
    "### Why Graph Representation?\n",
    " \n",
    " \n",
    "   * Sparse Parametrization\n",
    " \n",
    " \n",
    "   * Efficient Algorithms to reason from the data.\n",
    "\n",
    "\n",
    "### Two types of models:\n",
    "   \n",
    "   \n",
    "   * Bayesian Networks: Directed Models\n",
    "   \n",
    "   \n",
    "   * Markov Networks: Undirected Models\n",
    "\n",
    "\n",
    "### Where have these been used?\n",
    "   \n",
    "   \n",
    "   * Bayesian Networks: Medical Diagonsis\n",
    "   \n",
    "   \n",
    "   * Markov Networks: Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "**Q1**\n",
    "\n",
    "<img src=\"./files/Q1.png\" width=70%> </img>\n",
    "\n",
    "---------------\n",
    "**Q2.**\n",
    "\n",
    "<img src=\"./files/bayesian_net.png\" width=80%> </img>\n",
    "\n",
    "* Calculate:\n",
    "\n",
    "    $ P(d^0, i^1, g^3, s^1, l^1) $\n",
    "    \n",
    "    $ P(d^0, i^1,l^1) $\n",
    "    \n",
    "    $ P(G | i^0, d^0) $\n",
    "    \n",
    "    $ P(G | i^0) $\n",
    "---------------\n",
    "\n",
    "<img src=\"./files/Q2.png\" width=50%> </img>\n",
    "\n",
    "**Q3** Calculate P(Accident = 1|President = 1) and P(Accident = 1| Traffic=1, President = 1)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Bayesian Networks\n",
    "\n",
    "* Chapter 6 - Paramaeter learning: Binary Variables from <a href=\"https://www.amazon.com/Learning-Bayesian-Networks-Richard-Neapolitan/dp/0130125342\"> Learning Bayesian Networks</a>.\n",
    "\n",
    "### Structure learning v/s parameter learning\n",
    "\n",
    "* Two types of learning: Structure and Parameter learning. Both of them can be learnt from data. \n",
    "\n",
    "\n",
    "* Linear Regression. The use of regularization as a means to achieve structure learning and parameter learning simultaneously.\n",
    "\n",
    "\n",
    "* Here we'll use that we have structure given to us. All we need to do is learn the Parameters.\n",
    "\n",
    "\n",
    "### Beta distribution\n",
    "\n",
    "* $$ \\Gamma(x+1)=\\int_{0}^{\\infty}t^{x}e^{-t} $$ \n",
    "\n",
    "\n",
    "* $$ \\Gamma(x+1)=x \\; \\Gamma(x) $$\n",
    "\n",
    "\n",
    "* $$ B(x,y)=\\int_{0}^{1}t^{x-1}(1-t)^{y-1}  dt = \\frac{\\Gamma(x) \\; \\Gamma(y)}{\\Gamma(x+y)} $$\n",
    "\n",
    "\n",
    "* $$ Beta(x \\; | \\; \\alpha, \\beta) = \\frac{x^{\\alpha-1} \\; \\; (1-x)^{ \\; \\beta-1} }{ B(\\alpha,\\beta) } $$\n",
    "\n",
    "### Coin: Single node Bayesian Net\n",
    "\n",
    "* You have a coin. Assume that it is a biased coin and $ Side \\sim Bernoulli(f). $ \n",
    "\n",
    "<img src=\"./files/single_node.png\" width=30%/>\n",
    "\n",
    "* Your task is to learn $ f $. Let $ F $ be a random variable for the parameter $ f $. You have certain beliefs about the value of parameter $ f $ and you encode it in the prior $ F \\sim Beta(f; \\alpha, \\beta) $. \n",
    "\n",
    "\n",
    "* **Q.** Suppose your Prior is $ F \\sim Beta(f; a, b) $. What is $ P(Side=heads) $ ?  \n",
    "\n",
    "\n",
    "#### Theorem 6.1 \n",
    "\n",
    "Suppose $ X $ is a random variable with two values 0 and 1, $ F $ is another random variable such that \n",
    "\n",
    "$$ P(X = 1 | F = f) = f $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ P(X = 1) = E(F) $$\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "\n",
    "Now in order to learn the parameter, you let's model $ F $ along with the our variable $ X $.\n",
    "This is called as the **Augmented Bayesian Network** representation of our earlier Bayesian Net. \n",
    "\n",
    "Let's $ F $ follow the density function $ \\rho $ which is called the **prior density function of the parameters**.\n",
    "\n",
    "Now by the Bayes' rule we have $$ P(F=f\\;|\\;D=d) = \\frac{P(D=d \\;|\\; F=f) \\rho (f)} {P(D=d)} $$\n",
    "In this way, we'll learn the distribution of $ F $ from the data.\n",
    "\n",
    "<img src=\"./files/augmented_net.png\" width=20%/>\n",
    "\n",
    "--------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 6.2\n",
    "\n",
    "Suppose\n",
    "\n",
    "1. We have a set of random variables (or random vectors) $ D = { X^{(1)} , X^{(2)} , . . . X^{(M)} } $ such that each $ X^{(h)} $ has the same space.\n",
    "\n",
    "2. There is a random variable $ F $ with density function $ \\rho $ such that the $X^{(h)}$ s\n",
    "are I.I.D. for all values $ f $  of $ F $.\n",
    "\n",
    "Then $ D $ is called a sample of size $ M $ with parameter $ F $.\n",
    "\n",
    "Given a sample, the density function $ \\rho $ is called the **prior density function of the parameters** relative to the sample. It represents our prior belief concerning the unknown parameters. \n",
    "\n",
    "Given a sample, the marginal distribution of each $ X^{(h)} $ is the same. This distribution is called the **prior distribution** relative to the sample. It represents our prior belief concerning each trial.\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "#### Definition 6.3 \n",
    "\n",
    "Suppose we have a sample of size $ M $ such that\n",
    "1. each $ X^{(h)} $ has space  $ {0, 1} $;\n",
    "2. $ F $ has space $ [0, 1] $, and for $ 1 ≤ h ≤ M \\quad  P (X^{(h)} = 1| f) = f $. \n",
    "\n",
    "Then $D$ is called a binomial sample of size $M$ with parameter $F$.\n",
    "\n",
    "<img src=\"./files/sample.png\" width=50%/>\n",
    "\n",
    "\n",
    "#### Theorem 6.2 \n",
    "Suppose\n",
    "\n",
    "1. $D$ is a binomial sample of size $M$ with parameter $F$.\n",
    "2. We have a set of values $d = {x^{(1)} , x^{(2)} , . . . x^{(M)}}$ of the variables in $D$ (The set $d$ is called our data set (or simply data)).\n",
    "3. $s$ is the number of variables in $d$ equal to $1$.\n",
    "4. $t$ is the number of variables in $d$ equal to $0$.\n",
    "\n",
    "Then\n",
    "$ P(d) = E(F^s (1 − F)^t) $.\n",
    "\n",
    "** Proof **: Marginalization: $$ P(D = d) = \\int_0^1 P(D=d \\;|\\; F = f) \\rho(F = f) df $$  \n",
    "\n",
    "\n",
    "#### Corollary 6.2\n",
    "If the conditions in Theorem 6.2 hold, and $F$ has a beta distribution with parameters $ a, b, N = a + b$, then,\n",
    "\n",
    "$$ P(d) = \\frac{\\Gamma (N)}{\\Gamma(N + M)} \\frac{\\Gamma(a + s) \\Gamma (b + t)}{\\Gamma(a) \\Gamma(b)} $$\n",
    "\n",
    "**Proof**: Application of Lemma 6.4\n",
    "\n",
    "#### Lemma 6.4\n",
    "Suppose $F$ has a beta distribution with parameters $a, b$ and $ N = a + b$. $s$ and $t$ are two integers ≥ 0, and $M = s + t$. Then\n",
    "\n",
    "$$ E[F^s \\; [1 − F]^t] = \\frac{\\Gamma (N)}{\\Gamma(N + M)} \\frac{\\Gamma(a + s) \\Gamma (b + t)}{\\Gamma(a) \\Gamma(b)} $$\n",
    "\n",
    "#### Lemma 6.5 \n",
    "Suppose $F$ has a beta distribution with parameters $a, b$ and $ N = a + b$. $s$ and $t$ are two integers ≥ 0, and $M = s + t$. Then\n",
    "\n",
    "$$\\frac{f^s (1 − f)^t \\rho(f)}{E(F^s [1 − F]^t )} = Beta(f ; a + s, b + t) $$\n",
    "\n",
    "\n",
    "#### Theorem 6.3\n",
    "If the conditions in Theorem 6.2 hold, then\n",
    "$$ ρ(f|d) = \\frac{f^s (1 − f )^t ρ(f)}{E(F^s[1 − F ]^t)} $$\n",
    "\n",
    "where $\\rho(f|d) $ denotes the density function of $F$ conditional on $D = d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### Corollary 6.3 \n",
    "Suppose the conditions in Theorem 6.2 hold, and F has a $ Beta $ distribution with parameters $a, b$ and $ N = a + b$. That is, $$ ρ(f) = Beta(f ; a, b) $$\n",
    "\n",
    "Then, $$ ρ(f |d) = beta(f ; a + s, b + t) $$\n",
    "\n",
    "#### Theorem 6.4 \n",
    "Suppose the conditions in Theorem 6.2 hold, and we create a binomial sample of size $M + 1$ by adding another variable $X^{(M+1)} $ . Then if $ D $ is the binomial sample of size $ M $, the updated distribution relative to the sample and data $ d $ is given by \n",
    "\n",
    "\n",
    "$$P(X^{(M +1)} = 1| d) = E(F |d) $$\n",
    "\n",
    "#### Corollary 6.4 \n",
    "If the conditions in Theorem 6.4 hold and F has a beta distribution with parameters $a, b$ and N = a + b, then\n",
    "\n",
    "$$ P(X^{(M +1)} = 1| d) = \\frac{a + s}{N + M} = \\frac{a + s} {a + s + b + t} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next class\n",
    "\n",
    "* Marginalization (Some Problems)\n",
    "\n",
    "\n",
    "* Connections and building blocks of Bayesian Networks\n",
    "\n",
    "\n",
    "* Conditional Independences: d - Separation\n",
    "\n",
    "\n",
    "* Multinomial distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
