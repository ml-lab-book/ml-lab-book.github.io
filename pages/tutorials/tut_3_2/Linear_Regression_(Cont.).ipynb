{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Class:\n",
    "\n",
    "* MAP estimates\n",
    "\n",
    "* Beta distribution: Conjugate prior for a Bernoulli/Binomial likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "\n",
    "* $ MSE = Bias^2 + Variance $\n",
    "\n",
    "* Gauss Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "* Show $ E (b) = \\beta $ under the assumption that $ y_i = \\alpha + \\beta  x_i + \\epsilon_i$ where $ \\epsilon_i \\sim N(0, \\sigma^2)$. Thus show that least square estimators (a & b) are unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship of bias and variance with MSE\n",
    "\n",
    "Mean Square Error measure the \"average\" distance of the parameter estimate from its true value.  \n",
    "\n",
    "### Question 2\n",
    "Prove that:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\operatorname{MSE_{\\theta}}(\\hat{\\theta}) &= \\operatorname{E}_{X|\\theta} \\left [(\\hat{\\theta}-\\theta)^2 \\right ] \\\\ \n",
    "&= \\operatorname{Var}_{\\theta}(\\hat\\theta)+ \\operatorname{Bias}_{\\theta}(\\hat\\theta)^2\n",
    "\\end{align} $$\n",
    "\n",
    "[Hint Use the fact that $ Var(X) = E[X^2] - E[X]^2 $]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Markov Theorem\n",
    "\n",
    "If:\n",
    "\n",
    "* The expected average value of residuals is 0. ($ E(\\epsilon_i ) = 0 $)\n",
    "\n",
    "\n",
    "* The spread of residuals is constant and finite for all $ X_i (Var(\\epsilon_i  ) = \\sigma^2 $ )\n",
    "\n",
    "\n",
    "* There is no relationship amongst the residuals ( $ cov(\\epsilon_i , \\epsilon_j ) = 0 $)\n",
    "\n",
    "\n",
    "* There is no relationship between the residuals and the $ X_i (cov(X_i , \\epsilon_i ) = 0 $)\n",
    "\n",
    "\n",
    "Then, least square estimates have lowest variance amongst all linear unbiased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Our assumption that $ y_i = \\alpha + \\beta  x_i + \\epsilon_i$ where $ \\epsilon_i \\to N(0, \\sigma^2)$ is a special case of the Gauss Markov theorem. (We additionally, assume that the $epsilon_i$ are normal distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof: \n",
    "\n",
    "Let the regression line be $ Y = b_{0} + b_{1}X$.\n",
    "Least square estimates of coefficients are given by:\n",
    "$$\n",
    "b_{1} = \\frac{\\sum_{i}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}} = \\sum_{i}{K_{i}Y_{i}}\n",
    "$$\n",
    "\n",
    "where, \n",
    "$$\n",
    "K_{i} =  \\frac{(x_{i}-\\bar{x})}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}}\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "Y_{i} = y_{i}-\\bar{y}\n",
    "$$\n",
    "\n",
    "And the other coefficient is given by,\n",
    "$$\n",
    "b_{0} = \\bar{y} - b_{1}\\bar{x}\n",
    "$$\n",
    "\n",
    "Now first calculate variance of $b_{1}$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(b_{1})= & \\sigma^{2}(\\sum_{i}{K_{i}Y_{i}}) \\\\\n",
    "                 = & \\sum_{i}{K_{i}^{2}\\sigma^{2}(Y_{i})} .... (Why?)\\\\\n",
    "                 = & \\sigma^{2} * \\sum_{i}{\\frac{1}{(x_{i}-\\bar{x})^{2}}}\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\sigma^{2}$ is the variance of each $Y_{i}$. \\\\\n",
    "Now consider another estimator of $\\beta_{1}$ as $\\hat{\\beta_{1}}$.\\\\\n",
    "Let,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_{1}} = \\sum_{i}{c_{i}y_{i}}\n",
    "$$\n",
    "\n",
    "for some $c_{i}$.\n",
    "\n",
    "Now consider expected value and variance of this estimator.\n",
    "\n",
    "\\begin{align*}\n",
    "E(\\hat{\\beta_{1}}) = & \\sum_{i}{c_{i}E(y_{i})} \\\\\n",
    "                   = & \\sum_{i}{c_{i}E(\\beta_{0} + \\beta_{1}x_{i})} \\\\\n",
    "                   = & \\beta_{0}\\sum_{i}{c_{i}} + \\beta_{1}\\sum_{i}{c_{i}x_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "As $\\hat{\\beta_{1}}$ is an unbiased estimator, $E(\\hat{\\beta_{1}}) = \\beta_{1}$ for generic values of $x_{i}$. \\\\\n",
    "So from above expression we can get conditions on $c_{i}$'s as\\\\\n",
    "$\\sum_{i}{c_{i}}=0$ and \\\\\n",
    "$\\sum_{i}{c_{i}x_{i}}=1$\n",
    "\n",
    "Variance of the estimator is given by,\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sum_{i}{c_{i}\\sigma^{2}(y_{i})} \\\\\n",
    "= & \\sigma^{2}\\sum_{i}{c_{i}^{2}}\n",
    "\\end{align*}\n",
    "Let $c_{i} = K_{i} + d_{i}$ for some $d_{i}$. Then we can write,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sigma^{2}*(\\sum_{i}{( K_{i} + d_{i})^{2}}) \\\\\n",
    "= & \\sigma^{2}*(\\sum_{i}{K_{i}^{2}} + \\sum_{i}{d_{i}^{2}} + 2\\sum_{i}{K_{i}d_{i}}) \\\\\n",
    "= & \\sigma^{2}\\sum_{i}{K_{i}^{2}} + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2\\sigma^{2}\\sum_{i}{K_{i}d_{i}} \\\\\n",
    "= & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2\\sigma^{2}\\sum_{i}{K_{i}d_{i}}      .................. (\\sigma^{2}\\sum_{i}{K_{i}^{2}} = \\sigma^{2}(b_{1}))\n",
    "\\end{align*}\n",
    "Now consider the expression $\\sum_{i}{K_{i}d_{i}}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i}{K_{i}d_{i}} = & \\sum_{i}{K_{i}(c_{i} - K_{i})} \\\\\n",
    "= & \\sum_{i}{K_{i}c_{i}} - \\sum_{i}{K_{i}^{2}} \\\\\n",
    "= & \\sum_{i}{c_{i}(\\frac{(x_{i}-\\bar{x})}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}})} - \\frac{1}{(x_{i}-\\bar{x})^{2}} \\\\\n",
    "= & \\frac{\\sum_{i}{c_{i}x_{i}} - \\sum_{i}{c_{i}} - 1 }{\\sum_{i}{(x_{i}-\\bar{x})^{2}}}\n",
    "\\end{align*}\n",
    "We know that $\\sum_{i}{c_{i}x_{i}} = 1$ and $\\sum_{i}{c_{i}} = 0$ as $\\beta_{1}$ is an unbiased estimator (derived above). So substituting these values in above equation,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i}{K_{i}d_{i}} = & \\frac{1 - 0 - 1}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}} \\\\\n",
    "= & 0 ........................................(*)\n",
    "\\end{align*}\n",
    "Therefore we get,\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2*0 \\\\\n",
    "= & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} \\\\\n",
    "\\geq & \\sigma^{2}(b_{1})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, the least square estimate is the most **efficient** one amongst unbiased estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Summary\n",
    "\n",
    "\n",
    "* Minimizing the Mean squared loss function, L is the same as minimizing the (conditional) negative log likelihood (Maximizing the likelihood) under the assumption that $ Y|X \\sim \\alpha + \\beta X + \\epsilon ; \\quad \\epsilon \\sim N(0, \\sigma^2) $ \n",
    "\n",
    "<img src=\"./files/summary.png\" width=60%/>\n",
    "\n",
    "\n",
    "* Thus $$ b = \\frac{\\sum{x'_i y'_i}}{\\sum{x'_i}^2} = \\frac{sample \\; covariance \\; between \\; x \\; and \\; y} {sample \\; variance \\; of \\; x} $$ \n",
    "\n",
    "$$ a = \\bar{y} - b \\bar{x} $$ correspond to MLE estimates with the above assumption\n",
    "\n",
    "\n",
    "\n",
    "* Both the above estimates are unbiased.\n",
    "\n",
    "\n",
    "\n",
    "* The Gauss Markow theorem states that amongst unbiased estimates, the above estimates have the least variance and are thus the most efficient ones. **BLUE - Best Linear Unbiased Estimator **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading: https://people.eecs.berkeley.edu/~jegonzal/assets/slides/linear_regression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression\n",
    "\n",
    "http://faculty.cas.usf.edu/mbrannick/regression/Reg2IV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
