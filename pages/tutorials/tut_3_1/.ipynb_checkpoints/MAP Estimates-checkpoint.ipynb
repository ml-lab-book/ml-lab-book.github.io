{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP ESTIMATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics:\n",
    "\n",
    "* MAP Estimates for different types of hypothesis and data combinations\n",
    "* Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors, Bayes rule and MAP\n",
    "\n",
    "* Last class we dealt with maximising $ \\theta $ wrt $ L(x_1, x_2, ... x_n| \\theta) $ to obtain a point estimate for $ \\theta $ called as the MLE estimate.\n",
    "\n",
    "\n",
    "* Lets use the shorthand $ P(D|\\theta) $ for the likelihood term \n",
    "\n",
    "\n",
    "* Now let's assume that we have some beliefs on the value of $ \\theta $. For example, if you are tossing a coin and you believe that the coin is extremely biased to either heads or tails. If you represent your beliefs in terms of a probability distribution, you get the **prior distribution** on $ \\theta$. Let's call it $ P(\\theta)$\n",
    "\n",
    "\n",
    "* By Bayes' rule we have,\n",
    "\n",
    " $$ P( \\theta | D ) = \\frac{ P ( D | \\theta) P ( \\theta )}{ P( D )} $$\n",
    " \n",
    " This is called the posterior distribution of $ \\theta $\n",
    " \n",
    " \n",
    " \n",
    "* Maximum a posteriori estimate:\n",
    "\n",
    "$$ \\theta_{MAP} = arg max_{\\theta} P(\\theta | D) $$\n",
    "\n",
    "* Compare it to MLE:\n",
    "\n",
    "$$ \\theta_{MLE} = arg max_{\\theta} P( D | \\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis and Data\n",
    "\n",
    "* **Discrete hypothesis and Discrete data **\n",
    "\n",
    "\n",
    "* ** Continuous hypothesis and Discrete data **\n",
    "\n",
    "Homework (Slides)\n",
    "\n",
    "* Discrete hypothesis and Continuous data\n",
    "\n",
    "\n",
    "* Continuous hypothesis and Continuous data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete hypothesis and Discrete data\n",
    "\n",
    "<img src=\"files/dis_dis.png\" width = 80%/>\n",
    "\n",
    "\n",
    "**What is the MAP here? **\n",
    "\n",
    "**What is the MLE here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous hypothesis and Discrete data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/cont_dis.png\" width=80%/>\n",
    "\n",
    "Find the MAP of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta distribution\n",
    "\n",
    "* Let $ X \\sim Beta(\\alpha, \\beta). $ \n",
    "\n",
    " Then $$ P[X=x] = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1} \\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} $$\n",
    " \n",
    " where $$ \\Gamma(x) = \\int_{0}^{∞} t^{x-1} e^{-t} dt $$\n",
    " \n",
    " Remember $ \\Gamma(n) = (n - 1) ! $ for natural number n \n",
    "\n",
    "\n",
    "### Problems:\n",
    "\n",
    "* What is the value of X for which P[X=x] is maximized? What if $ \\alpha = \\beta $?\n",
    "\n",
    "\n",
    "* What is the expected value of X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution shape\n",
    "\n",
    "$ Beta(1, 1) $ \n",
    "\n",
    "* What happens when alpha increases keeping beta fixed?\n",
    "\n",
    "$ Beta (2, 1) $\n",
    "\n",
    "$ Beta (5, 1) $\n",
    "\n",
    "$ Beta(10, 1) $\n",
    "\n",
    "* When would you use this as a prior?\n",
    "\n",
    "$ Beta(0.5, 0.5) $\n",
    "\n",
    "\n",
    "<a href=\"https://homepage.divms.uiowa.edu/~mbognar/applets/beta.html\"> BETA DISTRIBUTION VISUALIZER </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Prior: $ Beta(\\alpha, \\beta) $\n",
    "\n",
    "Observed data: s heads, t tails\n",
    "\n",
    "* Find the posterior.\n",
    "\n",
    "* Find the MAP estimate of $ \\theta $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Why Beta distribution? Conjugate priors **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "\n",
    "* $ MSE = Bias^2 + Variance $\n",
    "\n",
    "* Gauss Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "* Show E (b) = β under the assumption that $ y_i = \\alpha + \\beta  x_i + \\epsilon_i$ where $ \\epsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "Thus show that least square estimators (a & b) are unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship of bias and variance with MSE\n",
    "\n",
    "### Question 2\n",
    "Prove that:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\operatorname{MSE_{\\theta}}(\\hat{\\theta}) &= \\operatorname{E}_{X|\\theta} \\left [(\\hat{\\theta}-\\theta)^2 \\right ] \\\\ \n",
    "&= \\operatorname{Var}_{\\theta}(\\hat\\theta)+ \\operatorname{Bias}_{\\theta}(\\hat\\theta)^2\n",
    "\\end{align} $$\n",
    "\n",
    "[Hint Use the fact that $ Var(X) = E[X^2] - E[X]^2 $]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Markov Theorem\n",
    "\n",
    "If:\n",
    "\n",
    "* The expected average value of residuals is 0. ($ E(\\epsilon_i ) = 0 $)\n",
    "\n",
    "\n",
    "* The spread of residuals is constant and finite for all $ X_i (Var(\\epsilon_i  ) = \\sigma^2 $ )\n",
    "\n",
    "\n",
    "* There is no relationship amongst the residuals ( $ cov(\\epsilon_i , \\epsilon_j ) = 0 $)\n",
    "\n",
    "\n",
    "* There is no relationship between the residuals and the $ X_i (cov(X_i , \\epsilon_i ) = 0 $)\n",
    "\n",
    "\n",
    "Then, least square estimates are unbiased and have lowest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Our assumption that $ y_i = \\alpha + \\beta  x_i + \\epsilon_i$ where $ \\epsilon_i \\to N(0, \\sigma^2)$ is a special case of the Gauss Markov theorem. (We additionally, assume that the $epsilon_i$ are normal distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof: \n",
    "\n",
    "Let the regression line be $ Y = b_{0} + b_{1}X$.\n",
    "Least square estimates of coefficients are given by:\n",
    "$$\n",
    "b_{1} = \\frac{\\sum_{i}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}} = \\sum_{i}{K_{i}Y_{i}}\n",
    "$$\n",
    "\n",
    "where, \n",
    "$$\n",
    "K_{i} =  \\frac{(x_{i}-\\bar{x})}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}}\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "Y_{i} = y_{i}-\\bar{y}\n",
    "$$\n",
    "\n",
    "And the other coefficient is given by,\n",
    "$$\n",
    "b_{0} = \\bar{y} - b_{1}\\bar{x}\n",
    "$$\n",
    "\n",
    "Now first calculate variance of $b_{1}$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(b_{1})= & \\sigma^{2}(\\sum_{i}{K_{i}Y_{i}}) \\\\\n",
    "                 = & \\sum_{i}{K_{i}^{2}\\sigma^{2}(Y_{i})} \\\\\n",
    "                 = & \\sigma^{2}(\\sum_{i}{\\frac{1}{(x_{i}-\\bar{x})^{2}}})\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\sigma^{2}$ is the variance of each $Y_{i}$. \\\\\n",
    "Now consider another estimator of $\\beta_{1}$ as $\\hat{\\beta_{1}}$.\\\\\n",
    "Let,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_{1}} = \\sum_{i}{c_{i}y_{i}}\n",
    "$$\n",
    "\n",
    "for some $c_{i}$.\n",
    "\n",
    "Now consider expected value and variance of this estimator.\n",
    "\n",
    "\\begin{align*}\n",
    "E(\\hat{\\beta_{1}}) = & \\sum_{i}{c_{i}E(y_{i})} \\\\\n",
    "                   = & \\sum_{i}{c_{i}E(\\beta_{0} + \\beta_{1}x_{i})} \\\\\n",
    "                   = & \\beta_{0}\\sum_{i}{c_{i}} + \\beta_{1}\\sum_{i}{c_{i}x_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "As $\\hat{\\beta_{1}}$ is an unbiased estimator, $E(\\hat{\\beta_{1}}) = \\beta_{1}$ for generic values of $x_{i}$. \\\\\n",
    "So from above expression we can get conditions on $c_{i}$'s as\\\\\n",
    "$\\sum_{i}{c_{i}}=0$ and \\\\\n",
    "$\\sum_{i}{c_{i}x_{i}}=1$\n",
    "\n",
    "Variance of the estimator is given by,\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sum_{i}{c_{i}\\sigma^{2}(y_{i})} \\\\\n",
    "= & \\sigma^{2}\\sum_{i}{c_{i}^{2}}\n",
    "\\end{align*}\n",
    "Let $c_{i} = K_{i} + d_{i}$ for some $d_{i}$. Then we can write,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sigma^{2}*(\\sum_{i}{( K_{i} + d_{i})^{2}}) \\\\\n",
    "= & \\sigma^{2}*(\\sum_{i}{K_{i}^{2}} + \\sum_{i}{d_{i}^{2}} + 2\\sum_{i}{K_{i}d_{i}}) \\\\\n",
    "= & \\sigma^{2}\\sum_{i}{K_{i}^{2}} + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2\\sigma^{2}\\sum_{i}{K_{i}d_{i}} \\\\\n",
    "= & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2\\sigma^{2}\\sum_{i}{K_{i}d_{i}}      .................. (\\sigma^{2}\\sum_{i}{K_{i}^{2}} = \\sigma^{2}(b_{1}))\n",
    "\\end{align*}\n",
    "Now consider the expression $\\sum_{i}{K_{i}d_{i}}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i}{K_{i}d_{i}} = & \\sum_{i}{K_{i}(c_{i} - K_{i})} \\\\\n",
    "= & \\sum_{i}{K_{i}c_{i}} - \\sum_{i}{K_{i}^{2}} \\\\\n",
    "= & \\sum_{i}{c_{i}(\\frac{(x_{i}-\\bar{x})}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}})} - \\frac{1}{(x_{i}-\\bar{x})^{2}} \\\\\n",
    "= & \\frac{\\sum_{i}{c_{i}x_{i}} - \\sum_{i}{c_{i}} - 1 }{\\sum_{i}{(x_{i}-\\bar{x})^{2}}}\n",
    "\\end{align*}\n",
    "We know that $\\sum_{i}{c_{i}x_{i}} = 1$ and $\\sum_{i}{c_{i}} = 0$ as $\\beta_{1}$ is an unbiased estimator (derived above). So substituting these values in above equation,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i}{K_{i}d_{i}} = & \\frac{1 - 0 - 1}{\\sum_{i}{(x_{i}-\\bar{x})^{2}}} \\\\\n",
    "= & 0 ........................................(*)\n",
    "\\end{align*}\n",
    "Therefore we get,\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^{2}(\\hat{\\beta_{1}}) = & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} + 2*0 \\\\\n",
    "= & \\sigma^{2}(b_{1}) + \\sigma^{2}\\sum_{i}{d_{i}^{2}} \\\\\n",
    "\\geq & \\sigma^{2}(b_{1})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, the least square estimate is the most **efficient** one amongst unbiased estimators.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
